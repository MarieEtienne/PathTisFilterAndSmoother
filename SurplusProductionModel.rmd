---
title: Mise en oeuvre du filtrage et lissage par méthodes particulaires sur un modèle de production de biomasse
author: "MP Etienne"
date: "January 30, 2017"
header-includes:
   - \usepackage{amsmath}
   - \usepackage{algorithm2e}
   - \newcommand{\Xbf}{\boldsymbol{X}}
   - \newcommand{\Ybf}{\boldsymbol{Y}}
   
output: pdf_document 
fig_caption: yes
---

L'inférence pour ce type de modèle est classiquement menée dans un cadre bayésien dans un logiciel type BUGS ou JAGS. Ce type de logiciel demande une intialisation cohérente des paramètres, ce qui peut être compliqué. Les algorithmes particulaires, peuvent permettre de construire cette initialisation.

Plus généralement, si on souhaite faire de l'estimation fréquentiste, les méthodes particulaires permettent de mettre en oeuvre un algorithme type Monte Carlo EM.


# Présentation de la problématique 
## Formalisme général
On note $n$ le nombre total d'observations, $\Ybf= Y_{0:n}=(Y_0, \ldots, Y_n)$ les observations (les indices d'abondance) et $\Xbf=X_{0:n}$ les variables non observées (la biomasse réelle). De plus, on possède l'historique des captures $C_1,\dots C_n$, supposées observées sans erreur.

Un modèle classique de dynamique de population avec prélèvement s'écrit
\begin{align}
X_{i+1} &= \left( X_i + r X_i\left(1- \frac{X_i}{K}\right) - C_i \right) \exp(\varepsilon_{i+1}), \quad \varepsilon_i\overset{i.i.d}{\sim}\mathcal{N}\left(-\frac{\sigma^2}{2}, \sigma^2\right)\\
Y_i\vert X_i &= q X_i \exp(\nu_{i}), \quad \nu_i\overset{i.i.d}{\sim}\mathcal{N}\left(-\frac{\sigma^2}{2}, \sigma^2\right)
\end{align}

Pour éviter des problèmes d'identifiabilité, la variance de l'erreur d'observation et celle de l'innovation du processus dynamique sont supposées égales.

On note $g_{\theta}(X_i,.)$ la loi de $Y_i$ conditionnelement à $X_i$, on note $m_{\theta}(X_{i}, .)$ le noyau de  transition de $X_i$ vers $X_{i+1}$ et $\nu_{\theta}()$, la loi initiale de $X_0$.
## Loi de filtrage

$\Xbf$ est un processus Markovien non linéaire observé indirectement au travers de $\Ybf$. On appelle loi de lissage la loi de $X_{k}\vert Y_{1:k}$. Supposons  les paramètres du modèle connus, la loi de filtrage est différente de la loi de $X_k$ puisque les $Y$ apportent de l'information sur l'état du processus $\Xbf$, autrement dit la connaissance de l'ensemble des indices d'abondance jusqu'à l'année $k$ donne une information sur l'état réel du stock à l'année $k$. 

Dans des cas très particulier (modèle linéaire Gaussien), on peut obtenir une forme explicite de la loi de filtrage (c'est le filtre de Kalman) mais ça n'est en général pas le cas. L'idée des  méthodes de filtrage particulaires est d'approcher par un échantillon de particules issues de cette loi.
La loi continue de $X_k$ va donc être approchée par une loi discrète, à valeurs dans un ensemble de particules $\xi_k^1,\dots,\xi_k^N$, associé à un ensemble de poids $\omega_k^1,\dots,\omega_k^N$ (avec la contrainte naturelle $\sum_{i=1}^N \omega_k^i =1$). Ainsi, pour toute fonction $f$, un estimateur de $\mathbb{E}(f(X_k)\vert Y_{1:k})$ sera donné par $$\sum_{i=1}^N \omega_k^if(\xi_{k}^i)$$

Afin de générer les couples $(\xi_k^i,\omega_k^i)$, on utilise des méthodes itératives. On commence par générer un échantillon approximation de la loi de  $X_1\vert Y_1$ et on va le propager pour obtenir un échantillon approchant la loi de $X_2\vert Y_{1:2}$, etc ....

## Loi de lissage
Si on a accès à l'ensemble des indices d'abondance jusqu'à l'année $n$, et que le but est de pouvoir utiliser au mieux cette information on va avoir envie d'utiliser la loi de lissage, c'est à dire la loi de $X_k\vert Y_{1:n}$ ou si on veut reconstruire les historiques de biomasses les plus probables, on peut vouloir des lois jointes du type $X_{k_1:k_2}\vert Y_{1:n}$.

Les méthodes particulaires peuvent être utilisées également pour obtenir des réalisations de ces lois. Après avoir obtenu les lois de filtrage jusqu'à l'instant $n$, on va pouvoir utiliser d'autres algorithmes pour obtenir des  échantillons de particules issus des lois de lissage.


# Mise en oeuvre d'un algorithme particulaire pour obtenir la loi de filtrage

## Rappel Importance sampling
Pour obtenir un échantillon issu d'une loi cible $f$ qu'on ne sait pas simuler, il est possible d'utiliser une loi instrumentale $g$ de la manière suivante.

\begin{algorithm}
\For{ i in 1:G}{
  Draw $Z_i\sim g(.)$\;
  Compute weight $w_i=f(Z_i)/g(Z_i)$\;
}
Compute $w_+=\sum_{i=1}^G w_i$\;
Define normalized weight $\tilde{w}_i=w_i/w_+$\;
\caption{Importance Sampling algorithm}
\end{algorithm}


On peut aussi ajouter une étape de resampling pour avoir un échantillon non pondéré
\begin{algorithm}
\For{ i in 1:G}{
  Draw $Z_i\sim g(.)$\;
  Compute weight $w_i=f(Z_i)/g(Z_i)$\;
}
Compute $w_+=\sum_{i=1}^G w_i$\;
Define normalized weight $\tilde{w}_i=w_i/w_+$\;
\For{ i in 1:G}{
  Sample $I_i$ in $\left\lbrace 1, \ldots,G\right\rbrace$ such that $P(I_i=j)=\tilde{W}_j$
  Define $\tilde{Z}_i= Z_{I_i}$
}
\caption{Importance Sampling-Resampling algorithm}
\end{algorithm}


## Importance sampling séquentiel
L'idée d'un filtrage particulaire est d'itérer ce processus d'importance sampling resampling pour obtenir des échantillons de la loi cible.

Il faut avoir une loi instrumentale $R(x, x')$ qui permet de propager une particule dans l'état $x$ à un instant donné vers une particule dans l'état $x\prime$ au temps suivant.
Si on dispose au temps $k$ d'un échantillon de la loi de filtrage, l'idée est de propager cet échantillon avec ce noyau $R$ et de pondérer chaque particule de manière appropriée. 
Si on s'intéresse à la loi de $X_{k+1}\vert X_k, Y_{k+1}$, on peut écrire
\begin{align*}
[X_{k+1}\vert X_k, Y_{1:k+1}] = & \frac{[X_{k+1} X_k, Y_{1:k+1}]}{[ X_k, Y_{1:k+1}]}\cr
& \frac{[Y_{k+1}\vert X_{k+1}][X_{k+1} \vert X_k][X_{k}\vert Y_{1:k}]}{[ X_k, Y_{1:k+1}]}\cr
&\propto \frac{[Y_{k+1}\vert X_{k+1}][X_{k+1} \vert X_k][X_{k}\vert Y_{1:k}]}{[ X_k, Y_{1:k+1}]}\cr
&\propto \frac{g(X_{k+1}, Y_{k+1}) m( X_k, X_{k+1})[X_{k}\vert Y_{1:k}]}{[ X_k, Y_{1:k+1}]}
\end{align*}


\begin{algorithm}
\For{ i in 1:G}{
  Simuler $X_0^i \sim r_0$\;
  Calculer $w_0^i=\nu(X_0^i)/r_0(X_0^i)$\;
}
\For{ k in 1:n}{
  \For{ i in 1:G}{
  Simuler $X_k^i \sim R_k(X_{k-1}^i,.)$\;
  Calculer $w_k^i=w_{k-1}^i \frac{m_{\theta}(X_{k-1}^i, X_k^i) g(X_k^i,Y_k)}{ R_k(X_{k-1}^i,X_{k}^i) }$\;
  }
}
\caption{Particle filter algorithm 1}
\end{algorithm}

# Application
## Simulation du modèle de production de biomasse
Le code de simulation du modèle est le suivant
```{r}

# Population Dynamic model simulation ----------------------------------
rm(list=ls())
set.seed(11)
nYear <- 30
biomass <- rep(NA, nYear)
capt <- rep(NA, nYear)
K <- 300 ## carrying capacity
r <- 1.02 ## recruitement rate
q <- 0.5
epsilon <- 10

sigmaBiomass <- 0.3
biomass[1] <- rnorm(1, mean=0.9*K, sd=K/10)
capt[1] <- runif(1,0.1,0.5)*biomass[1]
for( year in 2:nYear){
  Bk = biomass[year-1]
  capt[year-1] <- runif(1,0.1,0.5)*Bk
  #Captures are a random fraction of biomass
  innov <- rnorm(1, mean=-sigmaBiomass^2/2,sd=sigmaBiomass)
  biomass[year] <- (Bk+r*Bk*(1- Bk/K)-capt[year-1])*exp(innov)
  capt[year] <- runif(1,0.1,0.5)*biomass[year]
}
obs_error <- rnorm(nYear, mean=-sigmaBiomass^2/2, sd=sigmaBiomass)
abundanceIndex <- q * biomass*exp(obs_error)
#Calcul de la fenêtre des possible pour les Xs
sqrtD <- sqrt((1+r)^2-4*capt*r/K)
M0s <- (-(1+r)-sqrtD)/(-2*r/K)
m0s <- (-(1+r)+sqrtD)/(-2*r/K)
bounds <- cbind(m0s,M0s)
```
On trace les résultats obtenus. Les lignes grises sur le graphique représentent les frontières des possibles pour les valeurs des biomasses.
Le but est d'approcher les distributions de filtrage.
```{r}
par(mfrow=c(1,1))
plot(1:nYear,biomass, ylim=range(c(0,range(biomass),range(bounds))),
     main='Evolution de la biomasse',xlab="Année",type="b",pch=20,
     ylab="Masse",bty="n")
lines(1:(nYear),capt,type="b",col="red",pch=20)
legend("topright",pch=20,col=c("black","red"),
       legend = c("Biomasse","Captures"),
       bty="n",horiz = T)

lines(1:nYear, abundanceIndex, type='b',col="blue",pch=18,
      ylim=range(c(0,range(abundanceIndex))))
axis(side=4,at=pretty(range(abundanceIndex),n = 2),col = "blue",
     col.ticks = "blue",labels=F ,lwd.ticks=0)
mtext(side=4,text = "Indice d'abondance",col = "blue",at=150,line=1)
mtext(side=4,text = pretty(range(abundanceIndex),n=2),col = "blue",
      at=pretty(range(abundanceIndex),n=2))
matplot(bounds,lty=2,col="darkgrey",add=T,type="l")
```

## Codage du modèle
Commençons par coder les fonctions spécifiques du modèle, nécessaires au calcul des poids:

```{r}
mtheta<- function(x_old,x_new, capt_old,theta){
  r =theta$r; K=theta$K; sigmaBiomass = theta$sigmaBiomass;
  mn_innov =-sigmaBiomass^2/2
  dnorm(log(x_new), 
        mean=log(x_old+r*x_old*(1-x_old/K)- capt_old)-mn_innov, 
        sd=sigmaBiomass)
}
g<-function(x,y,theta){
  q =theta$q;sigmaBiomass=theta$sigmaBiomass;mn_innov =-sigmaBiomass^2/2
  dnorm(log(y), mean=log(q*x)-mn_innov,sd=sigmaBiomass)
}
```

## Codage des lois de propositions
Il s'agit ensuite de définir une loi de proposition.\\
Ce choix est essentiel, et s'il n'a pas d'influence en théorie, il aura une grande importance en pratique. 
De manière générale, il est souvent préférable que la loi de proposition soit flexible (donc explore bien l'espace des possibles) et dépende des observations. 
Dans l'exemple considéré ici, à l'étape $k$, on doit proposer différentes valeurs $\xi_k^i,~i=1,\dots,G$ pour la biomasse réelle. Ces valeurs doivent évidement être positives, mais aussi cohérentes avec les données de capture. Ce qui veut dire que pour tout $i$, l'inégalité suivante doit être vérifiée 
\begin{equation}
\xi_k^i+r\xi_k^i\left(1-\frac{\xi_k^i}{K}\right)>C_k
\label{eq:capt:cond}
\end{equation}
On peut vérifier que la condition \eqref{eq:capt:cond} est vérifiée si et seulement si:
\begin{align}
\Delta &: = (1+r)^2-4\frac{rC_k}{K}>0,\\
\text{Et, }~~& m_k:= -K\frac{-(1+r)+\sqrt{\Delta}}{2r} < \xi_k^i<M_k : =-K\frac{-(1+r)-\sqrt{\Delta}}{2r}\label{eq:def:bounds}
\end{align}

### Codage des lois initales
Ainsi, pour $k=0$, un choix possible de génération des particules est celui d'une loi Gaussienne tronquée, dépendant des observations $Y_0$ et $C_0$:
\begin{align}
\log \xi_0^i&\overset{i.i.d}{\sim} f_0\\
\text{Avec } f_0(x)&=\left\lbrace \begin{array}{lr}
\frac{\varphi_0(x)}{\int_{m_0}^{M_0 }\varphi_0(s)ds} & \text{si } m_0<\exp(x)<M_0\\
0&\text{sinon}
\end{array}\right.\label{eq:def:f0}
\end{align}
où $m_0$ et $M_0$ sont définis par l'équation \eqref{eq:def:bounds} et  $\varphi_0$ est la densité d'une loi Gaussienne $\mathcal{N}(\mu_0,\sigma^2_0)$ de paramètres
\begin{align*}
\mu_0&= \log Y_0 - \log q + \frac{\sigma^2}{2}\\
\sigma^2_0 &= \sigma^2
\end{align*}
Il faut noter ici que d'autres paramètrisation de la loi normale sont possibles. On pourrait par exemple choisir une variance plus grande, ou une moyenne ne dépendant pas de $Y_0$.
Les fonction de génération et la densité associées à l'équation \eqref{eq:def:f0} sont codées ci dessous: 
```{r}
#Fonction de génération
rf0 <- function(G, theta,Y0=NA,C0=NA){
    #Extraction des paramètres nécessaires
    r =theta$r; sigmaBiomass = theta$sigmaBiomass;q=theta$q;
    mn_innov = -sigmaBiomass^2/2;#Pourrait être défini autrement
    Delta <- (1+r)^2-4*C0*r/K
    sqrtD <- sqrt(Delta)
    M0 <- (-(1+r)-sqrtD)/(-2*r/K)
    m0 <- (-(1+r)+sqrtD)/(-2*r/K)
    X0 <- sapply(1:G,function(i){
      ok <- F
      while(!ok){#On simule jusqu'à tomber dans la zone 
        #acceptable au vu des captures
        cand <- exp(rnorm(1,log(Y0)-log(q)-mn_innov,
                          sigmaBiomass))
        ok <- (cand < M0) & (cand>m0)
      }
      return(cand)
    })
    return(X0)
}
df0 <- function(x0, theta,Y0=NA,C0=NA){
  #Extraction des paramètres nécessaires
  r =theta$r; sigmaBiomass = theta$sigmaBiomass;q=theta$q;
  mn_innov = -sigmaBiomass^2/2;#Pourrait être défini autrement
  Delta <- (1+r)^2-4*C0*r/K
  sqrtD <- sqrt(Delta)
  M0 <- (-(1+r)-sqrtD)/(-2*r/K)
  m0 <- (-(1+r)+sqrtD)/(-2*r/K)
  mn_ref <- log(Y0)-log(q)-mn_innov
  #Définition de la constante de normalisation
  norm_cst <- (pnorm(log(M0),mn_ref,sigmaBiomass)-
                 pnorm(log(m0),mn_ref,sigmaBiomass))
  dx <- dnorm(log(x0),mn_ref,sigmaBiomass)/norm_cst
  return(dx)
}
```

### Codage des lois de proposition
Il reste désormais à choisir une loi de proposition pour définir une particule $\xi_{k+1}^i$ à partir d'un parent $\xi_k^i$.
Il n'est pas nécessairement conseillé de prendre comme noyau de transition celui défini par le modèle. En effet, en pratique, le vrai jeu de paramètres n'est jamais connu. Pour un jeu de paramètres supposé, utiliser le noyau de transition du modèle peut alors être trop contraignant. Ainsi, on préfèrera ici une marche aléatoire autour des particules précédemment simulées, pondérée par l'observation $Y_{k+1}$\footnote{C'est exactement ici l'idée du filtre de Kalman}. Par exemple, si on considère une marche aléatoire Gaussienne sur le logarithme de la particule, et le processus d'observation, on a
\begin{align}
\log\xi_{k+1}^i\vert \log\xi_k^i& \sim \mathcal{N}(\log\xi_k^i,\sigma^2_{\text{RW}})
\intertext{Dans ce cas, on a alors}
\log \xi_{k+1}^i\vert \log\xi_k^i,\log Y_{k+1} &\sim \mathcal{N}(\mu_p,\sigma^2{p})\label{eq:prop:dist}
\intertext{Avec:}
\mu_p&= \sigma^2_p\left(\frac{\log \xi_{k}^i }{\sigma_{\text{RW}}^2} +\frac{\log Y_{k+1}^i -\log q +\sigma^2/2 }{\sigma^2}\right)\\
\sigma^2_p &= \frac{\sigma_{\text{RW}}^2+\sigma^2}{\sigma_{\text{RW}}^2\sigma^2}
\intertext{Comme pour le filtre de Kalman, cette loi se déduit de la décomposition}
p(\log \xi_{k+1}^i\vert \log\xi_k^i,\log Y_{k+1})&\propto p(\log\xi_{k+1}^i\vert\log \xi_k^i)p(\log Y_{k+1}\vert \log \xi_{k+1})
\end{align}
Encore une fois, si on veut prendre en compte les captures, il convient de tronquer la loi normale définie en \eqref{eq:prop:dist}, afin que pour tout $i$, $\xi_{k+1}^i$ satisfasse \eqref{eq:def:bounds}.

Le code résultant de ces distributions est donné par:
```{r}
rPropag <- function(x_old,theta,Y_new=NA,C_new=NA){
  sdRW = theta$sdRW; sigmaBiomass = theta$sigmaBiomass;
  mn_innov = -sigmaBiomass^2/2; q =theta$q;r=theta$r; 
  K = theta$K;
  sqrtD <- sqrt((1+r)^2-4*C_new*r/K)
  M_new <- (-(1+r)-sqrtD)/(-2*r/K)
  m_new <- (-(1+r)+sqrtD)/(-2*r/K)
  v_gau  <- (sdRW^2*sigmaBiomass^2)/(sdRW^2+sigmaBiomass^2)
  mn2    <- log(Y_new)-log(q)-mn_innov
  mn_gau <- v_gau*(log(x_old)/(sdRW^2)+mn2/(sigmaBiomass^2))
  Xnex <- sapply(mn_gau,function(mn){
    ok <- F
    while(!ok){
      cand <- exp(rnorm(1,mn,sqrt(v_gau)))
      ok <- (cand > m_new) & (cand < M_new)
    }
    return(cand)
  })
  return(Xnex)
}
dPropag <- function(x_new, x_old,theta,Y_new=NA,C_new=NA){
  sdRW = theta$sdRW; sigmaBiomass = theta$sigmaBiomass;
  mn_innov = -sigmaBiomass^2/2; q =theta$q;r=theta$r; 
  K = theta$K;
  sqrtD <- sqrt((1+r)^2-4*C_new*r/K)
  M_new <- (-(1+r)-sqrtD)/(-2*r/K)
  m_new <- (-(1+r)+sqrtD)/(-2*r/K)
  v_gau  <- (sdRW^2*sigmaBiomass^2)/(sdRW^2+sigmaBiomass^2)
  mn2    <- log(Y_new)-log(q)-mn_innov
  mn_gau <- v_gau*(log(x_old)/(sdRW^2)+mn2/(sigmaBiomass^2))
  dnex <- sapply(1:length(x_new),function(i){
    mn = mn_gau[i]
    (dnorm(log(x_new[i]),
           mn,
           sqrt(v_gau))
    /(pnorm(log(M_new),mn,sqrt(v_gau))
      -pnorm(log(m_new),mn,sqrt(v_gau))))
    #normalized by the area of possible zone
  })
  return(dnex)
}
```

# Un premier algorithme de filtrage
On effectue un premier algorithme de filtrage:
```{r}
#Initialisation
sdRW <- 10
theta=list(q=q, K=K, sigmaBiomass=sigmaBiomass,
           r=r,sdRW=sdRW)#Paramètres pour la génération de particules
#et calculs de poids
G <- 100#Nombre de particules
Xs1 <- matrix(NA, ncol=nYear, nrow=G)#Particules
Xs1[,1] <- rf0(G=G,theta= theta,
             Y0 = abundanceIndex[1],C0 = capt[1] )
ws1 <- matrix(NA, ncol=nYear, nrow=G)#Poids
ws1[,1] <- df0(x0=Xs1[,1],theta = theta,
              Y0 = abundanceIndex[1],C0 = capt[1])
ws1[,1] <- ws1[,1]/ws1[,1]
for(year in 2:nYear){
  Xs1[,year] <- rPropag(x_old = Xs1[,year-1],
                      theta = theta,
                      Y_new = abundanceIndex[year],
                      C_new = capt[year])#Génération de nvelles particules
  mth <- mtheta(x_old =Xs1[,year-1],
                x_new= Xs1[,year],
                theta= theta,
                capt_old=capt[year-1])#densité de transition du modele
  gfact <- g(Xs1[,year], abundanceIndex[year], theta)#Dnsité d'observation
  pfact <- dPropag(x_new = Xs1[,year],x_old= Xs1[,year-1],
                   theta = theta, Y_new = abundanceIndex[year],
                   C_new = capt[year])#Densité de la loi de proposition
  ws1[,year]=ws1[,year-1]*(mth*gfact/pfact)#Actualisation des poids
  ws1[,year] <-   ws1[,year]/sum(ws1[,year])#Normalisation des poids
}
```
On a donc généré un ensemble de particules et leurs poids associés. On peut commencer par regarder l'évolution des poids de filtrage:
```{r}
matplot(t(ws1),type="b",pch=20,xlab="Indice k",ylab="Poids",
        main="Evolution des poids de filtrage\n cas sans resampling",
        lty=1)
```
Il y a dégénérécence des poids de filtrage.
Pour palier ce problème, on effectue un resampling. À chaque étape, on duplique les particules ayant un "avenir prometteur".

# Filtrage avec resampling
Avant la génération de nouvelles particules, on retire les parents parmi les parents générés à l'étape précédente, selon leurs anciens poids. 
```{r,echo= -c(1:2)}
plot(1:nYear,biomass, ylim=range(c(range(biomass),range(bounds))),
     main='Evolution de la biomasse',xlab="Année",type="n",pch=20,
     ylab="Masse",bty="n")
matplot(bounds,lty=2,col="darkgrey",add=T,type="l")
#Initialisation, identique à précédemment
Xs2 <- matrix(NA, ncol=nYear, nrow=G)
Xs2[,1] <- rf0(G=G,theta= theta,
             Y0 = abundanceIndex[1],C0 = capt[1] )
ws2 <- matrix(NA, ncol=nYear, nrow=G)
ws2[,1] <- df0(x0=Xs2[,1],theta = theta,
                   Y0 = abundanceIndex[1],C0 = capt[1])
ws2[,1] <- ws2[,1]/sum(ws2[,1])
points(rep(1,G),Xs2[,1],cex=ws2[,1]/max(ws2[,1]),
       col="darkgreen",pch=20)

for(year in 2:nYear){
  inds_rspl <- sample(1:G,size=G,replace=T,
                      prob = ws2[,year-1])#Rééchantillonnage
  Xs2[,year] <- rPropag(x_old = Xs2[inds_rspl,year-1],
                      theta = theta,
                      Y_new = abundanceIndex[year],
                      C_new = capt[year])
  mth <- mtheta(x_old =Xs2[inds_rspl,year-1],
                x_new= Xs2[,year],
                theta= theta,
                capt_old=capt[year-1])
  gfact <- g(Xs2[,year], abundanceIndex[year], theta)
  pfact <- dPropag(x_new = Xs2[,year],x_old= Xs2[inds_rspl,year-1],
                   theta = theta, Y_new = abundanceIndex[year],
                   C_new = capt[year])
  ws2[,year]=(mth*gfact/pfact)
  ws2[,year] <-   ws2[,year]/sum(ws2[,year])
  points(rep(year,G), Xs2[,year],col="darkgreen",
         cex=ws2[,year]/max(ws2[,year]),pch=20)
}
lines(biomass,type="b",pch=20)
```
Regardons l'évolution des poids de filtrage
```{r}
matplot(t(ws2),type="b",pch=20,xlab="Indice k",ylab="Poids",
        main="Evolution des poids de filtrage\n cas sans resampling")
```
On peut visualiser les estimateurs à densité des distributions de filtrage. Estimateurs calculés à l'aide des particules et de leur poids de filtrage.
```{r}
par(mfrow=c(1,1))
k=10#Indice à représenter
plot(density(Xs2[,k],weights = ws2[,k]),
     main=bquote(paste('Loi de filtrage de '*'X'[.(k)])),
     xlab="Biomasse",ylab="Densité")
abline(v=biomass[k],lty=2)
points(Xs2[,k],rep(0,G) ,col="darkgreen",
        cex=ws2[,k]/max(ws2[,k]),pch=20)
```

# Lissage particulaire

Dans la partie précédente, nous avns approcher, pour tout $k$, la loi de $X_k\vert Y_{0:k}$. Intuitivement, on se doute que les observations $Y_{(k+1):n}$ non utilisées pourraient pourtant fournir une information sur la distribution de $X_k$. Ainsi, on va s'intéresser maintenant à la \textbf{loi de lissage} de $X_k$, à savoir celle de $X_k\vert Y_{0:n}$.

## Approche trajectorielle

On note ici $\xi_{0:n}^i$ la $i$-ème trajectoire de particules générée avec sa généalogie par l'algorithme de filtrage. À chaque trajectoire est associé le poids de filtrage final $\omega_n^i$. On note $\xi_{0:n}^i(k)$ la valeur de la trajectoire à l'instant $k$.
Lorsque qu'il n'y a pas de rééchantillonnage, cette trajectoire consiste en la succession des $\xi_k^i$, ainsi $\xi_{0:n}^i(k)=\xi_k^i$.
À partir de notre premier filtrage effectué plus haut, on peut obtenir ces trajectoires simulées.

```{r,echo=F,fig.cap="La trajectoire en noir a le plus grand poids, les trajectoires en blanc ont un poids presque nul."}
cols=ws1[,nYear]/max(ws1[,nYear])
matplot(t(Xs1),type="n",lwd=cols,lty=1,
        xlab="Année",ylab="Masse",
        main="Trajectoires particulaires\n Cas sans rééchantillonnage")
rect(par("usr")[1],par("usr")[3],
     par("usr")[2],par("usr")[4],col="lightgray")
matplot(t(Xs1)[,order(cols,decreasing = T)],add =T,
        type="l",lty=1,col=gray(1-cols[order(cols,decreasing = T)]))
```
<!-- \caption{Représentations des trajectoires particulaires associées au premier filtre. L'épaisseur représente le poids relatif de chaque trajectoire.} -->
