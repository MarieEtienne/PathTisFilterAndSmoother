\documentclass{beamer}

% Beamer style
%\usetheme[secheader]{Madrid}
\usetheme{CambridgeUS}
\usecolortheme[rgb={0.65,0.15,0.25}]{structure}
%\usefonttheme[onlymath]{serif}
\beamertemplatenavigationsymbolsempty


% Packages
\usepackage[utf8]{inputenc}
\usepackage{color}
% \usepackage{url}
\usepackage{dsfont, stmaryrd}
\usepackage{amsfonts,amsmath,amssymb,amsthm,ulem}
\usepackage{rotating}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{algorithm2e}
\usepackage[french]{babel}


\newcommand{\Xbf}{\boldsymbol{X}}
\newcommand{\Ybf}{\boldsymbol{Y}}


% Commands

\newcommand{\emphase}[1]{\textcolor{darkred}{#1}}
\newcommand{\emphaseBis}[1]{\textcolor{darkblue}{#1}}
\newcommand{\paragraph}[1]{\emphase{#1}}
\newcommand{\refer}[1]{\textcolor{blue}{\sl \cite{#1}}}
\newcommand{\Refer}[1]{\textcolor{blue}{\sl #1}}


\graphicspath{{figure/}}


\title[Filtrage et Lissage]{Mise en oeuvre du filtrage et lissage par méthodes particulaires sur un modèle de production de biomasse}
\author[MPE et PG]{MP Etienne et P. Gloaguen}

   
 
\begin{document}
 
\begin{frame}
  \titlepage
\end{frame}
% %--------------------------------------------------------------------
% %--------------------------------------------------------------------
\begin{frame}
\tableofcontents[subsectionstyle=hide]  
\end{frame}

\section{Présentation de la problématique} 
\subsection{Formalisme général}

% %--------------------------------------------------------------------
% %--------------------------------------------------------------------
\begin{frame}
\paragraph{Notations :}
\begin{itemize}
\item  $n$ le nombre total d'observations,
\item $\Ybf= Y_{0:n}=(Y_0, \ldots, Y_n)$ les observations, 
\item $\Xbf=X_{0:n}$ les variables non observées,
\end{itemize}
\pause
\paragraph{Le modèle de production de biomasse :}
{\small
\begin{align*}
X_{i+1} &= \left( X_i + r X_i\left(1- \frac{X_i}{K}\right) - C_i \right) \exp(\varepsilon_{i+1}),\\
&\quad \varepsilon_i\overset{i.i.d}{\sim}\mathcal{N}\left(-\frac{\sigma^2}{2}, \sigma^2\right)\\
Y_i\vert X_i &= q X_i \exp(\nu_{i}), \quad \nu_i\overset{i.i.d}{\sim}\mathcal{N}\left(-\frac{\sigma^2}{2}, \sigma^2\right)
\end{align*}
}
 
\paragraph{Le problème :}
{ Que peut-on dire de $\Xbf\vert \Ybf$, les captures $C_1,\dots C_n$ étant connues ?}
 \end{frame}
 
% %--------------------------------------------------------------------
% %--------------------------------------------------------------------
\begin{frame}
\paragraph{Notations :}
\begin{itemize}
\item  $n$ le nombre total d'observations,
\item $\Ybf= Y_{0:n}=(Y_0, \ldots, Y_n)$ les observations, 
\item $\Xbf=X_{0:n}$ les variables non observées,
\item $g_{\theta}(X_i,.)$ la loi de $Y_i$ conditionnelement à $X_i$
\item $m_{\theta}(X_{i}, .)$ le noyau de  transition de $X_i$ vers $X_{i+1}$,
\item $\nu_{\theta}()$, la loi initiale de $X_0$
\end{itemize}

% \begin{figure}
% \includegraphics[scale=0.5]{Dag3}
% \end{figure}
\end{frame}

 
\subsection{Loi de filtrage}

% %--------------------------------------------------------------------
% %--------------------------------------------------------------------
\begin{frame}
\frametitle{Loi de filtrage}
$\Xbf$ : processus Markovien non linéaire observé indirectement grâce à $\Ybf$. 
\bigskip 

\paragraph{Définition} La loi de filtrage est la loi de $X_{0:k}\vert Y_{0:k}$, notée $\phi_{0:k\vert 0:k}(.)$
\bigskip 

\emphase{Remarque :}
Cette loi  est différente de la loi de  $X_{0:k}$.\\
En général, elle n'a pas de forme analytique (sauf cas gaussien : Filtre de Kalman)
\pause
\begin{center}
\emphase{Recours à des méthodes de simulation}\\
\pause \medskip
\emphase{Méthodes particulaire}\\
\end{center}
\end{frame}

% %--------------------------------------------------------------------
% %--------------------------------------------------------------------

\begin{frame}
\frametitle{Méthodes particulaires}
\emphase{Idées des méthodes particulaires : }Approcher la loi de $\phi_{k\vert 0:k}(.)$ par un échantillon pondéré tiré dans cette loi : $\xi_k^1,\dots,\xi_k^N$, associé à un ensemble de poids $\omega_k^1,\dots,\omega_k^N$ (avec la contrainte naturelle $\sum_{i=1}^N \omega_k^i =1$). \bigskip

\pause
\emphase{Un exemple simple : échantillonnage d'importance :} On connaît la loi cible $g$ à une constante près.

<<IS1, echo=FALSE,fig.show='hide', fig.echo=FALSE>>=
set.seed(1)
abc <- seq(-10, 20, 0.1)
a <- 2 
b <- 2
plot(abc, dgamma(abc, shape = a, rate = b), 'l', xlim=c(-5, 10), xlab='', ylab='Density')
@

<<IS2, echo=FALSE,fig.show='hide', fig.echo=FALSE>>=
abc <- seq(-10, 20, 0.1)
a <- 2 
b <- 2
plot(abc, dgamma(abc, shape = a, rate = b), 'l', xlim=c(-5, 10), xlab='', ylab='Density')
lines(abc, dnorm(abc, mean=a/b, sd=2*sqrt(a/b^2)), col=2)
@

<<IS3, echo=FALSE,fig.show='hide', fig.echo=FALSE>>=
N <- 10
N10 <- N
part <- rnorm(N, mean=a/b, sd=2*sqrt(a/b^2))
part10 <- part
plot(abc, dgamma(abc, shape = a, rate = b), 'l', xlim=c(-5, 10), xlab='', ylab='Density')
lines(abc, dnorm(abc, mean=a/b, sd=2*sqrt(a/b^2)), col=2)
points(part, y=rep(0,N), pch=19, cex=0.8, col=2)
@


<<IS4, echo=FALSE,fig.show='hide', fig.echo=FALSE>>=
weight <- dgamma(part, shape = a, rate = b)/dnorm(part, mean=a/b, sd=2*sqrt(a/b^2))
weight <- weight/sum(weight)
weight10 <- weight
plot(abc, dgamma(abc, shape = a, rate = b), 'l', xlim=c(-5, 10), xlab='', ylab='Density')
lines(abc, dnorm(abc, mean=a/b, sd=2*sqrt(a/b^2)), col=2)
points(part, y=rep(0,N), pch=19, cex=0.8, col=2)
points(part, y=rep(0,N), pch=19, cex=4*weight, col=1)
@
<<IS5, echo=FALSE,fig.show='hide', fig.echo=FALSE>>=
plot(abc, dgamma(abc, shape = a, rate = b), 'l', xlim=c(-5, 10), xlab='', ylab='Density')
lines(abc, dnorm(abc, mean=a/b, sd=2*sqrt(a/b^2)), col=2)
points(part, y=rep(0,N), pch=19, cex=0.8, col=2)
points(part, y=rep(0,N), pch=19, cex=4*weight, col=1)
lines(density(part, weights = weight), col=3)
@

<<IS6, echo=FALSE,fig.show='hide', fig.echo=FALSE>>=
N <- 5000
part <- rnorm(N, mean=a/b, sd=2*sqrt(a/b^2))
plot(abc, dgamma(abc, shape = a, rate = b), 'l', xlim=c(-5, 10), xlab='', ylab='Density')
lines(abc, dnorm(abc, mean=a/b, sd=2*sqrt(a/b^2)), col=2)
points(part, y=rep(0,N), pch=19, cex=0.8, col=2)
weight <- dgamma(part, shape = a, rate = b)/dnorm(part, mean=a/b, sd=2*sqrt(a/b^2))
weight <- weight/sum(weight)
points(part, y=rep(0,N), pch=19, cex=weight, col=1)
lines(density(part, weights = weight), col=3)
@

<<IS7, echo=FALSE,fig.show='hide', fig.echo=FALSE>>=
plot(abc, dgamma(abc, shape = a, rate = b), 'l', xlim=c(-5, 10), xlab='', ylab='Density')
m <- max( dgamma(abc, shape = a, rate = b))
lines(abc, dnorm(abc, mean=a/b, sd=2*sqrt(a/b^2)), col=2)
points(part10, y=rep(0,N10), pch=19, cex=0.8, col=2)
weight <- dgamma(part, shape = a, rate = b)/dnorm(part, mean=a/b, sd=2*sqrt(a/b^2))
weight <- weight/sum(weight)
points(part10, y=rep(0,N10), pch=19, cex=4*weight, col=1)
lines(density(part10, weights = weight10), col=3)
partSelected <- sample(1:N10, size = N10, replace=T, prob=weight10)
p <- unique(part10[partSelected])
invisible(lapply(p,function(p_){
  np <- sum(part10[partSelected]==p_)
  points(x=rep(p_, np), y=seq(0, m/2, length.out=N10)[1:np], pch=18, col=3)
}))
@

% 
\begin{columns}
\begin{column}{0.45\textwidth}
 \only<2>{\includegraphics[scale=0.3]{IS1-1}}
 \only<3>{\includegraphics[scale=0.3]{IS2-1}}
 \only<4>{\includegraphics[scale=0.3]{IS3-1}}
 \only<5>{\includegraphics[scale=0.3]{IS4-1}}
 \only<6>{\includegraphics[scale=0.3]{IS5-1}}
 \only<7>{\includegraphics[scale=0.3]{IS7-1}}
 \end{column}
\begin{column}{0.45\textwidth}
 \only<6>{
    \small{
      \begin{algorithm}[H]
        \For{ i in 1:N}{
          Draw $Z^i\sim g(.)$\;
          Compute weight $w^i=f(Z^i)/g(Z^i)$\;}
        Compute $w_+=\sum_{i=1}^G w^i$\;
        Define normalized weight $\tilde{w}^i=w^i/w_+$\;
      \end{algorithm}
      }}
      \only<7>{
    \small{
      \begin{algorithm}[H]
        \For{ i in 1:N}{
          Draw $Z^i\sim g(.)$\;
          Compute weight $w^i=f(Z^i)/g(Z^i)$\;}
        Compute $w_+=\sum_{i=1}^G w^i$\;
        Define normalized weight $\tilde{w}^i=w^i/w_+$\;
        Resample according to $\tilde{w}^i$\;
      \end{algorithm}
      }}
 \end{column}
 \end{columns}
\end{frame}
% %--------------------------------------------------------------------
% %--------------------------------------------------------------------
 
\begin{frame}
\frametitle{Méthodes particulaires}
\emphase{Idées des méthodes particulaires : }Approcher la loi de $\phi_{k\vert 0:k}(.)$ par un échantillon pondéré tiré dans cette loi : $\xi_k^1,\dots,\xi_k^N$, associé à un ensemble de poids $\omega_k^1,\dots,\omega_k^N$ (avec la contrainte naturelle $\sum_{i=1}^N \omega_k^i =1$). \bigskip

\emphase{Méthode séquentielle :}  Pour échantillonner selon $\phi_{k\vert0:k}$, 
\begin{itemize}
\item Générer un échantillon $\xi_0^1,\dots,\xi_0^N$ selon $\phi_{0:0}$
\item Propager cet échantillon puis le pondérer pour obtenir un échantillon $\xi_1^1,\dots,\xi_1^N$
\item ...
\end{itemize}
\end{frame}

% %--------------------------------------------------------------------
% %--------------------------------------------------------------------
 
\begin{frame}
\end{frame}
\subsection{Loi de lissage}
\begin{frame}{Lissage particulaire} 
\begin{block}{Lissage}
On veut maintenant approcher la loi de 
\begin{itemize}
\item $X_k\vert Y_{0:n} := \phi_{k\vert n}$ (Lissage marginal);
\item $X_{k:\ell}\vert Y_{0:n} := \phi_{k:\ell\vert n}$ (Lissage joint).
\end{itemize}
Le but est (souvent) le calcul d'espérances de type: 
\begin{itemize}
\item $\mathbb{E}\left[f(X_k)\vert Y_{0:n}\right] := \phi_{k\vert n}[f]$;
\item $\mathbb{E}\left[f(X_{k:\ell})\vert Y_{0:n}\right] := \phi_{k:\ell \vert n}[f]$;
\textbf{Exemples :} Etape E de l'algorithme EM; Tracking;
\end{itemize}
\end{block}
\end{frame}
<<Liss1, echo=FALSE,fig.show='hide', fig.echo=FALSE>>=
set.seed(122)
par(mar=c(2,1,1,1))
xs <- rnorm(5)
ys <- xs+c(-0.1,-1.5,-3,-0.2,2)
ylim <- range(c(xs,ys))
plot(xs[1:3],type="n",pch=20,ylim=ylim,xlim=c(1,5),yaxt="n",
     ylab = "",cex.axis=2)
points(ys[1:3],type="b",pch=20,col="blue",cex=2)
legend("top",horiz = T,bty="n",pch=20,col=c("black","blue"),
       legend = c("X","Observ."),cex=2)
@
<<Liss2, echo=FALSE,fig.show='hide', fig.echo=FALSE>>=
set.seed(122)
par(mar=c(2,1,1,1))
xs <- rnorm(5)
ys <- xs+c(-0.1,-1.5,-3,-0.2,2)
ylim <- range(c(xs,ys))
plot(xs[1:3],type="n",pch=20,ylim=ylim,xlim=c(1,5),yaxt="n",
     ylab = "",cex.axis=2)
points(ys,type="b",pch=20,col="blue",cex=2)
legend("top",horiz = T,bty="n",pch=20,col=c("black","blue"),
       legend = c("X","Observ."),cex=2)
@
<<Liss3, echo=FALSE,fig.show='hide', fig.echo=FALSE>>=
set.seed(122)
par(mar=c(2,1,1,1))
xs <- rnorm(5)
ys <- xs+c(-0.1,-1.5,-3,-0.2,2)
ylim <- range(c(xs,ys))
plot(xs[1:3],type="n",pch=20,ylim=ylim,xlim=c(1,5),yaxt="n",
     ylab = "",cex.axis=2)
points(xs,type="b",pch=20,ylim=ylim,xlim=c(1,5))
points(ys,type="b",pch=20,col="blue",cex=2)
legend("top",horiz = T,bty="n",pch=20,col=c("black","blue"),
       legend = c("X","Observ."),cex=2)
@

\begin{frame}[plain]{Loi de lissage}
\textbf{Prise en compte de toute l'information disponible}
\begin{figure}
\centering
\only<1>{\includegraphics[scale=0.3]{Liss1-1}}
\only<2>{\includegraphics[scale=0.3]{Liss2-1}}
\only<3>{\includegraphics[scale=0.3]{Liss3-1}}
\caption{Exemple: Quelle est la loi $X_3$?}
\end{figure}
\textbf{Filtrage:} On ne considère que $Y_{1:3}$  \only<2-3>{\textbf{Lissage:} On considère $Y_{1:5}$} 
\end{frame}
\begin{frame}{Approche directe et naïve}
Pour chaque indice $i$, on considère la trajectoire simulée $\xi_{0:n}^i$ associée au poids $\omega_n^i$.\\
On fait alors l'approximation particulaire suivante:
$$ \phi_{k\vert n} \simeq \left(\xi_{0:n}^i(k),\omega_n^i \right)_{i=1,\dots,N}$$
\begin{figure}
\includegraphics[scale=0.3]{traj_first-1}
\caption{Représentation des trajectoires de particules pour le filtre sans rééchantillonnage. La couleur correspond au poids relatif de chaque trajectoire. Quand la trajectoire est blanche, son poids relatif est nul. Ici, une seule trajectoire contribuerait au calcul des lois de lissage.}
\end{figure}
\end{frame}
% \begin{frame}
% \begin{figure}
% \centering
% <<geneal,echo=F,fig.width=6,fig.height=6>>=
% par(mfrow=c(4,1),mar=c(1,1,1,1))
% matplot((nYear-2):nYear,ylab="",xlab="",xaxt="n",yaxt="n",col="black",
%        t(Geneal)[(nYear-2):nYear,],lty=1,type="b",pch=20,xlim=c(1,nYear))
% matplot((nYear-10):nYear,ylab="",xlab="",xaxt="n",yaxt="n",col="black",
%        t(Geneal)[(nYear-10):nYear,],lty=1,type="b",pch=20,xlim=c(1,nYear))
% matplot((nYear-20):nYear,ylab="",xlab="",xaxt="n",yaxt="n",col="black",
%        t(Geneal)[(nYear-20):nYear,],lty=1,type="b",pch=20,xlim=c(1,nYear))
% matplot((1):nYear,ylab="",xlab="",xaxt="n",yaxt="n",col="black",
%        t(Geneal)[(1):nYear,],lty=1,type="b",pch=20,xlim=c(1,nYear))
% @
% \end{figure}
% \subsection{Approche par simulation "backward"}
% Dans cette approche, on souhaite obtenir l'approximation d'une espérance de type: 
% \begin{equation}
% \mathbb{E}[f(X_{k:l})\vert Y_{0:n}]\label{eq:smooth:exp}.
% \end{equation}
% Pour ce faire,  on effectura $N$ simulations de trajectoires possible, notées $\{\xi_{0:n}^i\}_{i=1,\dots,N}$ et on approchera \eqref{eq:smooth:exp} par 
% \begin{equation}
% \frac{1}{N}\sum_{i=1}^{N} f\left(\xi_{0:n}^i(k:p)\right)
% \end{equation}
% L'idée de l'approche par simulation backward est de réutiliser l'ensemble des couples poids particules simulées lors du filtrage afin de simuler des trajectoires possibles.\\
% La procédure est intuitive, pour un point d'arrivée $(\xi_n^i,\omega_n^i)$ correspondant à la particule $i$, on simule son géniteur en tirant selon une multinomiale de poids
% \begin{equation}
% \Lambda_{n-1}^{j,i} = \frac{\omega_{n-1}^j q(\xi_{n-1}^j,\xi_{n}^i )}{\sum_{\ell=1}^N\omega_{n-1}^\ell q(\xi_{n-1}^\ell,\xi_{n}^i)}
% \label{eq:Lambdas}
% \end{equation}
% Aisni, le poids de chaque ancêtre est un compromis entre son poids de filtrage et la vraisemblance qu'il soit l'ancêtre du candidat considéré.\\ 
% On répète cette procédure itérativement afin d'obtenir une trajectoire.\\
% La fonction suivante permet la simulation
% \end{frame}
% \begin{frame}[plain]
% \begin{figure}
% \centering
% <<simu_Ntraj,echo=F,fig.height=5,fig.width=6,message=FALSE,fig.show=T,results="hide">>=
% Xb <- replicate(1,rbackward(Xs2,ws2)$traj)
% par(mfrow=c(3,1),mar=c(1,1,1,1))
% foo <- function(lag,Inds,Xs){
%   plot(rep(nYear,G),Xs2[,nYear],ylab="",xlab="",xaxt="n",yaxt="n",
%      type="p",pch=20,xlim=c(1,nYear),col="lightgray",
%      ylim=range(c(0,range(biomass),range(bounds))))
%   sapply(1:lag,function(i){
%     points(rep(nYear-i,G),Xs[,nYear-i],pch=20,col="lightgray")
%   })
%   matplot((nYear-lag):nYear,Inds[(nYear-lag):nYear,],
%         add=T,lty=1,type="b",pch=20)
% }
% foo(1,Xb,Xs2)
% foo(2,Xb,Xs2)
% foo(29,Xb,Xs2)
% @
% \caption{Simulation d'une trajectoire avec l'algorithme de simulation backward. Les ancêtres sont calculés à l'aide des poids de l'équation \eqref{eq:Lambdas}}
% \label{fig:back:sim:1traj}
% \end{figure}
% \end{frame}
% \begin{frame}[plain]
% \begin{figure}
% \centering
% <<simu_alltraj,echo=F,fig.height=5,fig.width=6,message=FALSE,fig.show=T,results="hide">>=
% Xb <- replicate(G,rbackward(Xs2,ws2)$traj)
% par(mfrow=c(1,1),mar=c(1,1,1,1))
% foo <- function(lag,Inds,Xs){
%   plot(rep(nYear,G),Xs2[,nYear],ylab="",xlab="",xaxt="n",yaxt="n",
%      type="p",pch=20,xlim=c(1,nYear),col="lightgray",
%      ylim=range(c(0,range(biomass),range(bounds))))
%   sapply(1:lag,function(i){
%     points(rep(nYear-i,G),Xs[,nYear-i],pch=20,col="lightgray")
%   })
%   matplot((nYear-lag):nYear,Inds[(nYear-lag):nYear,],
%         add=T,lty=1,type="b",pch=20)
% }
% foo(29,Xb,Xs2)
% lines(biomass,lwd=3,col="red")
% @
% \caption{Smulation de 30 trajectoires grâce à l'algorithme de backward simulation. La vraie trajectoire des biomasses est en rouge.}
% \label{fig:back:sim:alltraj}
% \end{figure}
% \end{frame}


\end{document}